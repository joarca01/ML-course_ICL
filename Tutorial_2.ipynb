{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joarca01/ML-course_ICL/blob/main/Tutorial_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F77Eo-ln22a4"
      },
      "source": [
        "# Tutorial 2 - Optimisation\n",
        "\n",
        "The purpose of this tutorial is:\n",
        "\n",
        "1.   Implementing the optimisation algorithms listed below:\n",
        "     \n",
        "        -   Gradient Descent Method\n",
        "        -   Gradient Descent Method with Momentum\n",
        "        -   Newton's Method\n",
        "\n",
        "2.   Comparing the convergence of the implemented algorithms.\n",
        "\n",
        "3.   Tackling a case study on projectile trajectory\n",
        "\n",
        "## Linear Regression\n",
        "***\n",
        "### Fundamentals\n",
        "\n",
        "Let us suppose that we would like to predict the price of a house. Inputs are the age of the building, total area, garage quality, and other information—that we believe affect a house’s worth. The output is the price of the house. _Such problems where the output is a number are regression problems_.\n",
        "\n",
        "The regression problem where the numeric outcome is predicted through an affine function is called linear regression.\n",
        "\n",
        "Input: $(X^i,y^i)_{i \\in \\{1,\\dotsc,m\\}}$ where\n",
        "\n",
        "- $m \\in \\mathbb{N}$ is the number of samples in the dataset\n",
        "\n",
        "- $n \\in \\mathbb{N}$ is the number of features\n",
        "\n",
        "- $X^i \\in \\mathbb{R}^n$  is an $n$ dimensional input vector for the $i^{\\textit{th}}$ sample\n",
        "\n",
        "- $y^i \\in \\mathbb{R}$ is the numeric output value associated with sample $i$\n",
        "\n",
        "\n",
        "Output: The goal in linear regression is to fit a hyperplane that explains the data most. In other words, we would like to estimate $\\beta := (\\beta_0, \\beta_1, \\dotsc, \\beta_n)$ (one coefficient for each feature plus a constant) that\n",
        "\n",
        "- predicts a new sample $y^{m+1}$ with $\\displaystyle y^{m+1}=\\beta_0 + \\sum_{i=1}^n \\beta_i X_i^{m+1}$\n",
        "    \n",
        "- minimizes the objective function of the form $\\displaystyle \\frac{1}{2m}\\sum_{i=1}^m \\bigg(y^i-\\Big(\\beta_0 + \\sum_{i=1}^n \\beta_i X_i^{m+1}\\Big)\\bigg)^2$ **(OLS problem)**\n",
        "\n",
        "\n",
        "\n",
        "### OLS in Matrix Form\n",
        "\n",
        "$$\\begin{align*}\n",
        "X=\\begin{bmatrix}\n",
        "1& x^1_{1} & \\dots  & x^1_{n}\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "1& x^m_{1} & \\dots  & x^m_{n}\n",
        "\\end{bmatrix}_{m\\times(n+1)}, \\quad y=\\begin{bmatrix} y^1 \\\\ \\vdots \\\\ y^m \\end{bmatrix}_{m \\times 1}, \\quad \\beta=\\begin{bmatrix} \\beta_0, \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{n} \\end{bmatrix}_{(n+1) \\times 1}\n",
        "\\end{align*} $$\n",
        "\n",
        "Then, OLS in matrix form can be formalized as the following quadratic optimization problem:\n",
        "\n",
        "$$\\begin{align}\n",
        "\\underset{x}{\\min} f(x) := \\frac{1}{2m}||Ax-b||_2^2\n",
        "\\end{align}$$\n",
        "\n",
        "***\n",
        "## Algorithms\n",
        "\n",
        "### Fundamentals\n",
        "\n",
        "Three algorithms that we are going to implement today are so-called iterative algorithms. They generate a sequence of points $\\beta^0, \\beta^1, \\dotsc$ in $\\mathbb{R}^n$ in the hope that this sequence will converge to a local minimizer. In our case, it converges to a global minimizer as we have a convex optimization problem. A typical rule for generating such a sequence would be to start with a vector $\\beta^0$, and then for $k \\geq 0$, move from iteration $k$ to $k+1$ by\n",
        "\n",
        "\\begin{align}\n",
        "\\beta^{k+1} = \\beta^k + \\alpha_k d_k\n",
        "\\end{align}\n",
        "\n",
        "in a way that we ensure $f(\\beta^{k+1})\\leq f(\\beta^k)$. The parameter $\\alpha_k \\in \\mathbb{R}$ is called the step length, while $d_k \\in \\mathbb{R}^n$ is the search direction.\n",
        "\n",
        "#### Gradient Descent and Newton's Method\n",
        "\n",
        "There are many ways in which the direction $d_k$ can be chosen. If we have\n",
        "\n",
        "\\begin{align}\n",
        "d_k = - \\nabla f(\\beta^k)\n",
        "\\end{align}\n",
        "\n",
        "then we take a step in the direction of steepest descent and the method is called gradient descent. If there is second-order information available, then we can take steps of the form\n",
        "\n",
        "\\begin{align}\n",
        "d_k = - \\nabla^2 f(\\beta^k)^{-1} \\nabla f(\\beta^k)\n",
        "\\end{align}\n",
        "\n",
        "The resulting method is called Newton's method. If applicable, Newton's method tends to converge faster to a solution, but the computation at each step is more expensive.\n",
        "\n",
        "#### Gradient Descent with Momentum\n",
        "\n",
        "Unlike the first two algorithms, this method carries over some momentum from previous iterations: Instead of only taking into account the current iterations, the gradient step is based on a combination current and previous steps. In this setting, we update $\\beta^{k+1}$ by\n",
        "\n",
        "\\begin{align}\n",
        "\\beta^{k+1} = \\beta^k + \\alpha_k \\theta_k\n",
        "\\end{align}\n",
        "where\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_k = -\\Big(\\nabla f(\\beta^k)+\\eta \\theta_{k-1}\\Big)\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7NfHRyy22a6"
      },
      "source": [
        "## Let's start coding!\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yVOdd1u522a7"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Libraries for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Libraries for downloading a dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb6Bludq22a7"
      },
      "source": [
        "We can now define the functions to be used later in the implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yC2VPqEq22a8"
      },
      "outputs": [],
      "source": [
        "def predict(X, betas):\n",
        "    '''\n",
        "    Returns the predictions for a given input matrix X and coefficients betas\n",
        "    using PyTorch tensor operations.\n",
        "\n",
        "    @params:\n",
        "    X: 2D PyTorch tensor with dim (n_sample, n_feature+1)\n",
        "    betas: 1D PyTorch tensor with dim (n_feature+1,)\n",
        "\n",
        "    @return:\n",
        "    1D PyTorch tensor with dim (n_sample,)\n",
        "    '''\n",
        "\n",
        "    ######## TODO: ########\n",
        "    # Assume the matrix has already been augmented with a column of ones.\n",
        "    return torch.matmul(X,betas)\n",
        "\n",
        "\n",
        "def OLS_objective_value(X, y, betas):\n",
        "    '''\n",
        "    Returns the objective function value for a given input matrix X,\n",
        "    an output vector y, and coefficients betas using PyTorch tensor operations.\n",
        "\n",
        "    @params:\n",
        "    X: 2D PyTorch tensor with dim (n_sample, n_feature+1)\n",
        "    y: 1D PyTorch tensor with dim (n_sample,)\n",
        "    betas: 1D PyTorch tensor with dim (n_feature+1,)\n",
        "\n",
        "    @return:\n",
        "    Singleton tensor\n",
        "    '''\n",
        "    m = y.shape[0]             # Number of samples\n",
        "    y_hat = predict(X, betas)  # Predictions\n",
        "\n",
        "    ######## TODO: ########\n",
        "    val = torch.linalg.vector_norm(y - y_hat)*(1/(2*m))  # OLS objective function\n",
        "\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K61ApO3q22a8",
        "outputId": "629af67c-c5b6-40b4-ba50-22edd19724d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great, it should work\n"
          ]
        }
      ],
      "source": [
        "# Testing the model\n",
        "# torch.mv() is matrix-vector multiplication\n",
        "\n",
        "# Creating testing data\n",
        "X_testing = torch.randn(100,2)\n",
        "betas_testing = torch.ones(2)\n",
        "y_testing = 2 * X_testing.mv(betas_testing) + torch.randn(100)*0.01\n",
        "\n",
        "# Testing the predict function\n",
        "predictions_testing = predict(X_testing, betas_testing)\n",
        "\n",
        "# Check that the shape of the output of predict function is correct\n",
        "if len(predictions_testing.shape) != 1:\n",
        "  raise RuntimeError('Predict function does not return 1d tensor')\n",
        "\n",
        "# Testing the OLS_objective_value function\n",
        "OLS_obj_value_testing = OLS_objective_value(X_testing, y_testing, betas_testing)\n",
        "\n",
        "# Check that the output of OLS_objective_value function is a singleton tensor\n",
        "if len(OLS_obj_value_testing.shape) != 1 and OLS_obj_value_testing.numel() !=1:\n",
        "  raise RuntimeError('OLS_objective_value function does ot return singleton tensor')\n",
        "\n",
        "# print(predictions_test)\n",
        "# print(OLS_obj_value_test)\n",
        "\n",
        "print('Great, it should work')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF0tfxhF22a8"
      },
      "source": [
        "We then create a class called `Optimizer` from which the classes designed for each algorithm are inherited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vo1ccXC922a8"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    '''\n",
        "    It is the class from which classes associated with each algorithm\n",
        "    are inherited. This includes the common attributes and methods\n",
        "    shared by all algorithms.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, X, y, learning_rate, init_betas=None, tolerance=1e-6):\n",
        "        '''\n",
        "        Initializes the Optimizer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D torch Tensor\n",
        "            Input data with dimensions (n_samples, n_features).\n",
        "        y : 1D torch Tensor\n",
        "            Output/target data with dimension (n_samples,).\n",
        "        learning_rate : float\n",
        "            Learning rate or regularization parameter.\n",
        "        init_betas : 1D torch Tensor, optional\n",
        "            Initial coefficients for the model with dimension (n_features+1,).\n",
        "            Defaults to a zero tensor if not provided.\n",
        "        tolerence : float, optional\n",
        "            Tolerance for convergence criteria.\n",
        "            Defaults to 1e-6 if not provided.\n",
        "        '''\n",
        "\n",
        "        # Storing initial data\n",
        "        self.X_original = X\n",
        "        self.y = y\n",
        "\n",
        "        # Extracting dimensions\n",
        "        self.n_sample, self.n_feature = X.shape\n",
        "\n",
        "        ######## TODO: ########\n",
        "        # Adding a column of ones for the intercept term\n",
        "        self.X = torch.cat([torch.ones(X.shape[0], 1, device=X.device), X], dim=1)\n",
        "\n",
        "        ######## TODO: ########\n",
        "        # Storing hyperparameters\n",
        "        self.lr = learning_rate\n",
        "        self.ToL = tolerance\n",
        "\n",
        "        # Initialize coefficients\n",
        "        if init_betas is not None:\n",
        "            self.betas = init_betas\n",
        "        else:\n",
        "            self.betas = torch.zeros((self.n_feature + 1))\n",
        "\n",
        "        # Algorithm history\n",
        "        self.obj_history = [OLS_objective_value(self.X, self.y, self.betas)]\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------------------------------\n",
        "    # UTILS FUNCTIONS FOR PLOTTING - DO NOT CHANGE (unless you know what you are doing)\n",
        "    # ---------------------------------------------------------------------------------\n",
        "    def plot_learned_model(self, algorithm_name):\n",
        "\n",
        "        '''\n",
        "        Visualizes the learned model so that\n",
        "        if n=1, it shows the fitted line\n",
        "        if n=2, it shows the fitted hyperplane\n",
        "        otherwise, it gives us an error.\n",
        "        '''\n",
        "\n",
        "        if self.n_feature > 2:\n",
        "            raise ValueError('You can visualize only in 2D and 3D!')\n",
        "\n",
        "        if self.n_feature == 1:\n",
        "            y_hat = predict(self.X, self.betas).numpy()\n",
        "            fig, ax = plt.subplots(figsize=(8, 4))\n",
        "            ax.plot(self.X_original.numpy(), y_hat, color='k', label='Learned Model')\n",
        "            ax.scatter(self.X_original.numpy(), self.y.numpy(), edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data')\n",
        "            ax.set_ylabel('Predicted Value', fontsize=13)\n",
        "            ax.set_xlabel('Feature Value', fontsize=13)\n",
        "            ax.legend(facecolor='white', fontsize=11)\n",
        "            ax.set_title(f'Learned Model with {algorithm_name}', fontsize=15)\n",
        "            fig.tight_layout()\n",
        "\n",
        "        elif self.n_feature == 2:\n",
        "            x1 = self.X_original[:, 0].numpy()\n",
        "            x2 = self.X_original[:, 1].numpy()\n",
        "            y_np = self.y.numpy().reshape((self.n_sample,))\n",
        "\n",
        "            x1_range = np.linspace(np.min(x1), np.max(x1), 30)\n",
        "            x2_range = np.linspace(np.min(x2), np.max(x2), 30)\n",
        "            xx_pred, yy_pred = np.meshgrid(x1_range, x2_range)\n",
        "            model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
        "            model_viz_b = np.c_[np.ones((model_viz.shape[0],1)), model_viz]\n",
        "\n",
        "            response = predict(torch.from_numpy(model_viz_b).float(), self.betas).numpy()\n",
        "\n",
        "            plt.style.use('default')\n",
        "            fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "            ax1 = fig.add_subplot(131, projection='3d')\n",
        "            ax2 = fig.add_subplot(132, projection='3d')\n",
        "            ax3 = fig.add_subplot(133, projection='3d')\n",
        "\n",
        "            axes = [ax1, ax2, ax3]\n",
        "\n",
        "            for ax in axes:\n",
        "                ax.plot(x1, x2, y_np, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
        "                ax.scatter(xx_pred.flatten(), yy_pred.flatten(), response, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
        "                ax.set_xlabel('Feature 1', fontsize=12)\n",
        "                ax.set_ylabel('Feature 2', fontsize=12)\n",
        "                ax.set_zlabel('Prediction', fontsize=12)\n",
        "                ax.locator_params(nbins=4, axis='x')\n",
        "                ax.locator_params(nbins=5, axis='x')\n",
        "\n",
        "            ax1.view_init(elev=28, azim=120)\n",
        "            ax2.view_init(elev=4, azim=114)\n",
        "            ax3.view_init(elev=60, azim=165)\n",
        "\n",
        "            fig.suptitle(f'Learned Model with {algorithm_name}', fontsize=15)\n",
        "            fig.tight_layout()\n",
        "\n",
        "    def plot_loss(self):\n",
        "\n",
        "        if len(self.obj_history) == 1:\n",
        "            raise ValueError('No history to plot.')\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        ax.plot(self.obj_history, color='k')\n",
        "        ax.set_ylabel('Objective Function Value')\n",
        "        ax.set_xlabel('Iteration')\n",
        "        ax.grid()\n",
        "        ax.set_title(f'Convergence of {self.algorithm_name}')\n",
        "        fig.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRyFPwd622a9",
        "outputId": "9e55230a-3622-48a9-93e5-08b5494611f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great, it should work\n"
          ]
        }
      ],
      "source": [
        "# Testing the Optimizer class\n",
        "\n",
        "opt_testing = Optimizer(X_testing, y_testing, 0.01)\n",
        "\n",
        "if type(opt_testing.X) != torch.Tensor:\n",
        "  raise RuntimeError('X should be a tensor')\n",
        "\n",
        "if type(opt_testing.lr) != float:\n",
        "  raise RuntimeError('learning_rate should be a float')\n",
        "\n",
        "if opt_testing.X.shape != torch.Size([100,3]):\n",
        "  raise RuntimeError('X should have 100 rows and 3 columns')\n",
        "\n",
        "if not torch.equal(opt_testing.X[:, 0], torch.ones(100)):\n",
        "  raise RuntimeError('X should have a column of ones')\n",
        "\n",
        "print('Great, it should work')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs1vTu2J22a9"
      },
      "source": [
        "We can now introduce the classes one by one for each algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YMZ82P722a9"
      },
      "source": [
        "### Standard Gradient Descent (no autograd)\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nyc1RmY_22a9"
      },
      "outputs": [],
      "source": [
        "class GradientDescent(Optimizer):\n",
        "\n",
        "    def __init__(self, X, y, learning_rate, init_betas=None, tolerance=1e-6):\n",
        "        '''\n",
        "        Constructor for the Gradient Descent optimizer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D PyTorch tensor\n",
        "            Input data with dimensions (n_samples, n_features).\n",
        "        y : 1D PyTorch tensor\n",
        "            Target/output data with dimension (n_samples,).\n",
        "        learning_rate : float\n",
        "            Learning rate for the optimization algorithm.\n",
        "        '''\n",
        "\n",
        "        super().__init__(X, y, learning_rate, init_betas=init_betas, tolerance=tolerance)\n",
        "        self.algorithm_name = 'Gradient Descent'\n",
        "\n",
        "    def run(self, max_iter=10000):\n",
        "        '''\n",
        "        Runs the Gradient Descent optimization algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_iter : int, optional\n",
        "            Maximum number of iterations for the optimization process.\n",
        "        '''\n",
        "        n_iter = 0\n",
        "        while n_iter < max_iter:\n",
        "\n",
        "            ######## TODO: ########\n",
        "            # Gradient computation\n",
        "            y_hat = ...\n",
        "            gradient = ... # use torch.mv = matrix-vector product\n",
        "\n",
        "            # Update parameters\n",
        "            x_old = self.betas.clone()\n",
        "            self.betas -= self.lr * gradient\n",
        "\n",
        "            # History update\n",
        "            self.obj_history.append(OLS_objective_value(self.X, self.y, self.betas).item())\n",
        "\n",
        "            n_iter += 1\n",
        "\n",
        "            # Convergence check\n",
        "            if torch.sum(torch.abs(self.betas - x_old)) <= self.ToL:\n",
        "                print(f'{self.algorithm_name} has converged in {n_iter} iterations')\n",
        "                break\n",
        "\n",
        "        if n_iter == max_iter:\n",
        "            print(f'{self.algorithm_name} has reached the maximum number of iterations ({max_iter})')\n",
        "\n",
        "    def plot_learned_model(self):\n",
        "        # Inherited from 'Optimizer' class with the algorithm name\n",
        "        super().plot_learned_model(self.algorithm_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUHX8nGa22a-"
      },
      "outputs": [],
      "source": [
        "# Testing on toy data\n",
        "gd_testing = GradientDescent(X_testing, y_testing, learning_rate=0.1)\n",
        "gd_testing.run()\n",
        "gd_testing.plot_learned_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xV8D_B22a-"
      },
      "source": [
        "### Introducing momentum to gradient descent (no autograd)\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecjZVioy22a-"
      },
      "outputs": [],
      "source": [
        "class GradientDescentMomentum(Optimizer):\n",
        "\n",
        "    def __init__(self, X, y, learning_rate, eta, init_betas=None, tolerance=1e-6):\n",
        "        '''\n",
        "        Constructor for the Gradient Descent with Momentum optimizer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D PyTorch tensor\n",
        "            Input data with dimensions (n_samples, n_features).\n",
        "        y : 1D PyTorch tensor\n",
        "            Target/output data with dimension (n_samples,).\n",
        "        learning_rate : float\n",
        "            Learning rate for the optimization algorithm.\n",
        "        eta : float\n",
        "            Momentum factor, indicating the importance of the previous iteration's parameter update.\n",
        "        '''\n",
        "\n",
        "        super().__init__(X, y, learning_rate, init_betas=init_betas, tolerance=tolerance)\n",
        "        self.eta = eta\n",
        "        self.algorithm_name = 'Gradient Descent Momentum'\n",
        "\n",
        "    def run(self, max_iter=20000):\n",
        "\n",
        "        # Initialize number of iterations\n",
        "        n_iter = 0\n",
        "\n",
        "        # Initialize momentum\n",
        "        theta = torch.zeros((self.n_feature + 1))\n",
        "\n",
        "        while n_iter < max_iter:\n",
        "\n",
        "            # Storing the values of betas for tolerance check.\n",
        "            betas_old = self.betas.clone()\n",
        "\n",
        "            ######## TODO: ########\n",
        "            # Gradient computation\n",
        "            y_hat = ...\n",
        "            theta_old = theta.clone()\n",
        "\n",
        "            ######## TODO: ########\n",
        "            theta = ...\n",
        "            self.betas -= ...\n",
        "\n",
        "            # Here initialising theta_old wasn't necessary but it is good practice beacuse if we were using autograd\n",
        "            # it would have given the error 'inplace operation on a tensor that is being grad tracked'\n",
        "\n",
        "            # Convergence check and n_iter and history update\n",
        "            self.obj_history.append(OLS_objective_value(self.X, self.y, self.betas).item())\n",
        "            n_iter += 1\n",
        "\n",
        "            if torch.sum(torch.abs(self.betas - betas_old)) <= self.ToL:\n",
        "                print(f'{self.algorithm_name} has converged in {n_iter} iterations')\n",
        "                break\n",
        "\n",
        "        if n_iter == max_iter:\n",
        "            print(f'{self.algorithm_name} has reached the maximum number of iterations ({max_iter})')\n",
        "\n",
        "    def plot_learned_model(self):\n",
        "        super().plot_learned_model(self.algorithm_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmfd5tNq22a-"
      },
      "outputs": [],
      "source": [
        "# Testing on toy data\n",
        "gd_momentum_testing = GradientDescentMomentum(X_testing, y_testing, learning_rate=0.1, eta=0.9)\n",
        "gd_momentum_testing.run()\n",
        "gd_momentum_testing.plot_learned_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYQhYYX522a-"
      },
      "source": [
        "### Convergence comparison\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D0b15ov22a-"
      },
      "source": [
        "#### California housing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhebGWTC22a_"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------\n",
        "# DATASET DOWNLOAD DO NOT CHANGE\n",
        "# ------------------------------------------------\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data[:10000], california.target[:10000]\n",
        "y = y.squeeze()\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj3ALN3I22a_"
      },
      "outputs": [],
      "source": [
        "# Running the algorithms\n",
        "\n",
        "gd_std = GradientDescent(X_tensor, y_tensor, learning_rate=0.001)\n",
        "gd_std.run(max_iter = 20000)\n",
        "gd_mom = GradientDescentMomentum(X_tensor, y_tensor, learning_rate=0.001, eta=0.95)\n",
        "gd_mom.run(max_iter = 10000)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(gd_std.obj_history, label='GD', color='blue')\n",
        "plt.plot(gd_mom.obj_history, label='GD with Momentum', color='orange')\n",
        "plt.scatter(len(gd_std.obj_history) - 1, gd_std.obj_history[-1], marker='o', color='blue')\n",
        "plt.scatter(len(gd_mom.obj_history) - 1, gd_mom.obj_history[-1], marker='o', color='orange')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Objective Function Value')\n",
        "plt.xlabel('Iteration')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7ifFe5Q22a_"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------\n",
        "# INTERACTIVE PLOT - DO NOT CHANGE THE CODE BELOW\n",
        "# ------------------------------------------------\n",
        "def run_gd_and_plot(learning_rate, eta_mom):\n",
        "    # Running standard GD\n",
        "    gd_std = GradientDescent(X_tensor, y_tensor, learning_rate=learning_rate)\n",
        "    gd_std.run(max_iter=20000)\n",
        "\n",
        "    # Running GD with Momentum\n",
        "    gd_mom = GradientDescentMomentum(X_tensor, y_tensor, learning_rate=learning_rate, eta=eta_mom)\n",
        "    gd_mom.run(max_iter=20000)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(gd_std.obj_history, label='GD', color='blue')\n",
        "    plt.plot(gd_mom.obj_history, label='GD with Momentum', color='orange')\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel('Objective Function Value')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Creating interactive widgets\n",
        "lr_list = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 0.5, 1]\n",
        "\n",
        "learning_rate_slider = widgets.SelectionSlider(options=lr_list, description='LR:')\n",
        "eta_mom_slider = widgets.FloatSlider(value=0.95, min=0, max=1, step=0.05, description='Momentum:')\n",
        "\n",
        "# Displaying the interactive plot\n",
        "widgets.interactive(run_gd_and_plot, learning_rate=learning_rate_slider, eta_mom=eta_mom_slider)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N19Oejdx22a_"
      },
      "source": [
        "In the cell above, you can try changing the leanring rate and the momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALbojyYB22a_"
      },
      "source": [
        "### Newton's method\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCsf6PcF22a_"
      },
      "source": [
        "Before writing the class implementing the Newton's method, we need to understand the following proposition.\n",
        "\n",
        "**Preposition 2:**  In use of the OLS problem, Newton's method converges in a single step independent of the initial point $\\beta^0$ when the step size is set to $1$.\n",
        "\n",
        "**Proof:** We have that\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla f(\\beta) &= \\frac{1}{m}[X^T X \\beta - X^T y]\\\\\n",
        "\\nabla^2 f(\\beta) &= \\frac{1}{m} X^T X\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "By inserting those gradient and Hessian computations into the update method of Newton's method, we get\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\beta^{k+1} &= \\beta^k - m (X^T X)^{-1}\\frac{1}{m}(X^T X \\beta^k - X^Ty)\\\\\n",
        "        &= (X^T X)^{-1} X^T y\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "This is the standard, closed form expression for the OLS coefficients. Therefore, no matter what we choose for the initial guess $\\beta^0$, we'll have the correct solution at $\\beta^1$ after a single iteration.\n",
        "\n",
        "Furthermore, this is a stationary point. Notice that the expression for $\\beta^{t+1}$ doesn't depend on $\\beta^t$, so the solution won't change if we continue beyond one iteration. This indicates that Newton's method converges in a single step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuycLXxx22bA"
      },
      "outputs": [],
      "source": [
        "class Newton(Optimizer):\n",
        "\n",
        "    def __init__(self, X, y, learning_rate, init_betas=None, tolerance=1e-6):\n",
        "        '''\n",
        "        Constructor for the Gradient Descent optimizer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D PyTorch tensor\n",
        "            Input data with dimensions (n_samples, n_features).\n",
        "        y : 1D PyTorch tensor\n",
        "            Target/output data with dimension (n_samples,).\n",
        "        learning_rate : float\n",
        "            Learning rate for the optimization algorithm.\n",
        "        '''\n",
        "\n",
        "        super().__init__(X, y, learning_rate, init_betas=init_betas, tolerance=tolerance)\n",
        "        self.algorithm_name = 'Newton method'\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        ######## TODO: ########\n",
        "        self.betas = ...\n",
        "\n",
        "        # History update\n",
        "        self.obj_history.append(OLS_objective_value(self.X, self.y, self.betas).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifOg_VRZ22bA"
      },
      "outputs": [],
      "source": [
        "# Run Newton's method\n",
        "newton = Newton(X_tensor, y_tensor, learning_rate=1)\n",
        "newton.run()\n",
        "\n",
        "print('Gradinet Descent Final Objective Value: ', ' '*8, round(gd_std.obj_history[-1], 9))\n",
        "print('Gradinet Descent Momentum Final Objective Value: ', round(gd_mom.obj_history[-1], 9))\n",
        "print('Newton Method Final Objective Value: ', ' '*11, round(newton.obj_history[-1], 9))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w-WztXI22bA"
      },
      "source": [
        "### Case Study\n",
        "***\n",
        "\n",
        "In this case study, we explore the optimization of a projectile's launch parameters to hit a specific target. We consider the effects of gravity and air resistance and use gradient descent with momentum to find the optimal launch velocity and angle.\n",
        "\n",
        "#### Physical Model\n",
        "\n",
        "The motion of the projectile is influenced by gravity and air resistance. The position of the projectile at time \\( t \\) is given by:\n",
        "\n",
        "$$\n",
        "x(t) = (v_0 \\cos(\\theta) - k v_0^2) \\cdot t\n",
        "$$\n",
        "\n",
        "$$\n",
        "y(t) = (v_0 \\sin(\\theta) - k v_0^2) \\cdot t - \\frac{1}{2} g t^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ v_0 $ is the initial velocity.\n",
        "- $ \\theta $ is the launch angle.\n",
        "- $ g $ is the acceleration due to gravity (approximated as 9.81 m/s\\(^2\\)).\n",
        "- $ k $ is the air resistance coefficient.\n",
        "- $ x(t) $ and $ y(t) $ are the horizontal and vertical positions of the projectile at time \\( t \\), respectively.\n",
        "\n",
        "#### Loss Function\n",
        "\n",
        "The loss function is defined as the squared distance between the projectile's landing point and the target:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\sqrt{\\left( x(t_{\\text{land}}) - x_{\\text{target}} \\right)^2 + \\left( y(t_{\\text{land}}) - y_{\\text{target}} \\right)^2}\n",
        "$$\n",
        "\n",
        "where $ t_{\\text{land}} $ is the time when the projectile lands (i.e., when $ y(t) = 0 $).\n",
        "\n",
        "#### Optimization with Gradient Descent and Momentum\n",
        "\n",
        "We use gradient descent with momentum to optimize the initial velocity $ v_0 $ and launch angle $ \\theta $. Recall that the update rules are:\n",
        "\n",
        "$$\n",
        "v_{\\text{new}} = v - \\alpha \\Big(\\nabla_{v} \\text{Loss} + \\eta \\cdot \\Delta v_{\\text{prev}}\\Big)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{\\text{new}} = \\theta - \\alpha\\Big(\\nabla_{\\theta} \\text{Loss} + \\eta \\cdot \\Delta \\theta_{\\text{prev}}\\Big)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ \\alpha $ is the learning rate.\n",
        "- $ \\eta $ is the momentum factor.\n",
        "- $ \\Delta v_{\\text{prev}} $ and $ \\Delta \\theta_{\\text{prev}} $ are the updates from the previous iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxc3AjWK22bA"
      },
      "outputs": [],
      "source": [
        "def projectile_motion(velocity, angle, time, air_resistance_coefficient=0.005):\n",
        "\n",
        "    g = 9.81  # Gravity\n",
        "\n",
        "    ######## TODO: ########\n",
        "    # Break down the velocity into its x and y components\n",
        "    vx = ...\n",
        "    vy = ...\n",
        "\n",
        "    # Air resistance\n",
        "    air_resistance = air_resistance_coefficient * velocity**2\n",
        "\n",
        "    ######## TODO: ########\n",
        "    # Calculate position at time t with air resistance\n",
        "    x = ...\n",
        "    y = ...\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def trajectory_loss(velocity, angle, target_x, target_y):\n",
        "\n",
        "    time_to_land = 2 * velocity * torch.sin(angle) / 9.81\n",
        "    x, y = projectile_motion(velocity, angle, time_to_land, air_resistance_coefficient=0.005)\n",
        "\n",
        "    ######## TODO: ########\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZeKgLTJ22bA"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "velocity = torch.tensor(10.0, requires_grad=True)\n",
        "angle = torch.tensor(torch.pi / 8, requires_grad=True)\n",
        "\n",
        "projectile_motion_testing = projectile_motion(velocity, angle, time=torch.tensor(1.0))\n",
        "\n",
        "if round(projectile_motion_testing[0].item(),4) != 8.7388 and round(projectile_motion_testing[1].item(),4) != -1.5782:\n",
        "    raise RuntimeError('projectile_motion function does not return correct values')\n",
        "\n",
        "trajectory_loss_testing = trajectory_loss(velocity, angle, target_x=torch.tensor(8.0), target_y=torch.tensor(0.0))\n",
        "\n",
        "if round(trajectory_loss_testing.item(), 4) != 1.2448:\n",
        "    raise RuntimeError('trajectory_loss function does not return correct values')\n",
        "\n",
        "print('Great, it should work')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKEN4T_722bA"
      },
      "outputs": [],
      "source": [
        "class GradientDescentMomentum2:\n",
        "\n",
        "    def __init__(self,\n",
        "                 velocity : float,\n",
        "                 angle    : float,\n",
        "                 target_x : float,\n",
        "                 target_y : float,\n",
        "                 eta      : float,\n",
        "                 learning_rate : float):\n",
        "\n",
        "        ######## TODO: ########\n",
        "        # Initialising tensors that require grads\n",
        "        self.velocity = ...\n",
        "        self.angle    = ...\n",
        "\n",
        "        ######## TODO: ########\n",
        "        # Initialising other parameters as floats\n",
        "        self.target_x = ...\n",
        "        self.target_y = ...\n",
        "        self.learning_rate = ...\n",
        "        self.eta = ...\n",
        "\n",
        "        # Initialising velocity and angle updates\n",
        "        self.velocity_update = torch.zeros_like(self.velocity)\n",
        "        self.angle_update = torch.zeros_like(self.angle)\n",
        "\n",
        "        # For plotting\n",
        "        self.velocity_history = []\n",
        "        self.angle_history = []\n",
        "        self.loss_history = []\n",
        "\n",
        "\n",
        "    def run(self, max_iter=10000, tolerance=1e-6):\n",
        "\n",
        "        # Initialising number of iterations\n",
        "        n_iter = 0\n",
        "\n",
        "        while n_iter < max_iter:\n",
        "\n",
        "            # Store old parameter values for convergence check\n",
        "            velocity_old = self.velocity.clone()\n",
        "            angle_old = self.angle.clone()\n",
        "\n",
        "            ######## TODO: ########\n",
        "            # Compute loss and perform backward pass\n",
        "            loss = ...\n",
        "            ...\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                ######## TODO: ########\n",
        "                # Update velocity and angle updates\n",
        "                self.velocity_update = ...\n",
        "                self.angle_update = ...\n",
        "\n",
        "                ######## TODO: ########\n",
        "                # Update parameters with the learning rate\n",
        "                self.velocity -= ...\n",
        "                self.angle -= ...\n",
        "\n",
        "                # Apply constraints\n",
        "                self.angle.clamp_(0, torch.pi / 2)\n",
        "                self.velocity.clamp_(min=0)\n",
        "\n",
        "                # Reset gradients\n",
        "                self.velocity.grad.zero_()\n",
        "                self.angle.grad.zero_()\n",
        "\n",
        "                # History update\n",
        "                self.velocity_history.append(self.velocity.item())\n",
        "                self.angle_history.append(self.angle.item())\n",
        "                self.loss_history.append(loss.item())\n",
        "\n",
        "            # Convergence check\n",
        "            if torch.abs(self.velocity - velocity_old) <= tolerance and torch.abs(self.angle - angle_old) <= tolerance:\n",
        "                print(f'Converged in {n_iter} iterations')\n",
        "                break\n",
        "\n",
        "            n_iter += 1\n",
        "\n",
        "        if n_iter == max_iter:\n",
        "            print(f'Reached the maximum number of iterations ({max_iter})')\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # UTILS FUNCTIONS FOR PLOTTING DO NOT CHANGE\n",
        "    # ------------------------------------------------\n",
        "\n",
        "    def plot_trajectory(self):\n",
        "        # Plotting the trajectory in parameter space\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Trajectory in velocity-angle space\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.velocity_history, self.angle_history, marker='o', color='blue')\n",
        "        plt.xlabel('Velocity')\n",
        "        plt.ylabel('Angle')\n",
        "        plt.grid()\n",
        "        plt.title('Trajectory in Parameter Space')\n",
        "\n",
        "        # Trajectory in loss space\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.loss_history, marker='o', color='blue')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid()\n",
        "        plt.title('Loss During Optimization')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Optional: Implement a function to plot the 3D loss surface and trajectory\n",
        "    def plot_loss_surface(self, velocity_range, angle_range, trajectory_loss_fn):\n",
        "        velocity_grid, angle_grid = torch.meshgrid(velocity_range, angle_range)\n",
        "        loss_surface = torch.zeros_like(velocity_grid)\n",
        "\n",
        "        for i in range(velocity_grid.size(0)):\n",
        "            for j in range(velocity_grid.size(1)):\n",
        "                loss_surface[i, j] = trajectory_loss_fn(velocity_grid[i, j], angle_grid[i, j], self.target_x, self.target_y)\n",
        "\n",
        "        # Plotting\n",
        "\n",
        "        plt.style.use('default')\n",
        "        fig = plt.figure(figsize=(18, 8))\n",
        "\n",
        "        ax1 = fig.add_subplot(121, projection='3d')\n",
        "        ax2 = fig.add_subplot(122, projection='3d')\n",
        "\n",
        "        axes = [ax1, ax2]\n",
        "\n",
        "        for ax in axes:\n",
        "            #ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.plot_surface(velocity_grid.numpy(), angle_grid.numpy(), loss_surface.numpy(), alpha=0.3, cmap='viridis')\n",
        "            ax.plot(self.velocity_history, self.angle_history, self.loss_history, color='r', marker='o', markersize=3)\n",
        "            ax.set_xlabel('Velocity')\n",
        "            ax.set_ylabel('Angle')\n",
        "            ax.set_zlabel('Loss')\n",
        "            ax.set_title('Optimization Path in Loss Landscape')\n",
        "\n",
        "        ax2.view_init(elev=20, azim=80)\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyK5KA3u22bB"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL WITH THIS PARAMS THAT DEFINITELY WORK - then try to change them and see what happens\n",
        "# (if you change them too much make sure to change the ranges in the plot_loss_surface function - last line of this cell)\n",
        "\n",
        "# Initial parameters\n",
        "init_velocity = 8.0\n",
        "init_angle    = torch.pi / 2  # 45 degrees\n",
        "target_x      = 50.0\n",
        "target_y      = 4.0\n",
        "learning_rate = 0.001\n",
        "eta           = 0.8\n",
        "\n",
        "optimizer = GradientDescentMomentum2(velocity=init_velocity, angle=init_angle, target_x=target_x,\n",
        "                                    target_y=target_y, learning_rate=learning_rate, eta=eta)\n",
        "\n",
        "optimizer.run()\n",
        "optimizer.plot_trajectory()\n",
        "optimizer.plot_loss_surface(torch.linspace(0, 35, 100), torch.linspace(0, np.pi / 2, 100), trajectory_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EBLrynP22bB"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}